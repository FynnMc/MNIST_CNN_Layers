{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Data Loadin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by loading in training and test sets and checking that the dimensions appear correct. Data is coming from the MNIST set which is known to be reliable and well formed so checking data completeness is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 42000, columns: 785\n",
      "Rows: 28000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "traindf = pd.read_csv(\"train.csv\")\n",
    "testdf = pd.read_csv(\"test.csv\")\n",
    "index = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "print(\"Rows: %d, columns: %d\" % (traindf.shape[0], traindf.shape[1]))\n",
    "print('Rows: %d, columns: %d' % (testdf.shape[0], testdf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = traindf.drop(labels='label', axis=1)\n",
    "y_data = traindf['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data, Normalisation and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    (33600, 784) (33600,)\n",
      "Validation:  (8400, 784) (8400,)\n"
     ]
    }
   ],
   "source": [
    "# Split data for Cross-Validation\n",
    "X_train, y_train = X_data.iloc[:33600], y_data.iloc[:33600]\n",
    "X_valid, y_valid = X_data.iloc[33600:], y_data.iloc[33600:]\n",
    "\n",
    "print('Training:   ', X_train.shape, y_train.shape)\n",
    "print('Validation: ', X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27d82a5d6d8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_norm = X_train/255\n",
    "X_valid_norm = X_valid/255\n",
    "X_test_norm = testdf/255\n",
    "\n",
    "del X_data, y_data, X_train, X_valid\n",
    "\n",
    "# Example of how preprocessed data appears post normalisation\n",
    "visual = X_train_norm.values.reshape(-1,28,28,1)\n",
    "sns.set()\n",
    "plt.imshow(visual[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a CNN using Layers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opt to predefine minibatch function outside of Convolutional class\n",
    "\n",
    "def batch_generator(X, y, batch_size=64, \n",
    "                    shuffle=False, random_seed=None):\n",
    "    \n",
    "    idx = np.arange(y.shape[0])\n",
    "    \n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X[i:i+batch_size, :], y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Conv class from creating model objects which are then trained using the train module. Comments inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ConvNN(object):\n",
    "    \n",
    "    def __init__(self, batchsize=64,\n",
    "                 epochs=20, learning_rate=1e-4, \n",
    "                 dropout_rate=0.5,\n",
    "                 shuffle=True, random_seed=None):\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.shuffle = shuffle\n",
    "                \n",
    "        # Intialise a new TF Graph    \n",
    "            \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            # Set Random-Seed:\n",
    "            tf.set_random_seed(random_seed)\n",
    "            \n",
    "            # Build Network\n",
    "            self.build()\n",
    "\n",
    "            # Variable Initialisation\n",
    "            self.init_op = \\\n",
    "                tf.global_variables_initializer()\n",
    "\n",
    "            # Saver to save model once built\n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "        # Once graph initialised pass to Session\n",
    "        self.sess = tf.Session(graph=g)\n",
    "                \n",
    "    # Build Model\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        # Placeholders for X and y:\n",
    "        tf_x = tf.placeholder(tf.float32, \n",
    "                              shape=[None, 784],\n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, \n",
    "                              shape=[None],\n",
    "                              name='tf_y')\n",
    "        # Only want to apply dropout to training set, not test set\n",
    "        # Hence create boolean placeholder to identify if using training set\n",
    "        is_train = tf.placeholder(tf.bool, \n",
    "                              shape=(),\n",
    "                              name='is_train')\n",
    "\n",
    "        # Numpy allows us to leave a dimension blank by using -1\n",
    "        # Numpy will then  figure out what the unknown dimension should be\n",
    "       \n",
    "        # Reshape x to a 4D tensor: \n",
    "        # [batchsize, width, height, 1]\n",
    "        # Number of input channels = 1\n",
    "        tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n",
    "                              name='input_x_2dimages')\n",
    "        # Inbuild TF One-hot encoding:\n",
    "        tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n",
    "                              dtype=tf.float32,\n",
    "                              name='input_y_onehot')\n",
    "\n",
    "        \n",
    "        # Implementation using Layers API\n",
    "         \n",
    "        # 1st layer: Conv_1\n",
    "        # Default padding is 'VALID' i.e. No padding\n",
    "        # Filters = 32 means we have 32 output feature maps\n",
    "        # 28x28x1 -> 24x24x32 via 5x5x32 weight matrices\n",
    "        h1 = tf.layers.conv2d(tf_x_image, \n",
    "                              kernel_size=(5, 5), \n",
    "                              filters=32, \n",
    "                              activation=tf.nn.relu)\n",
    "        # MaxPooling\n",
    "        # Pooling w/stride produces 12x12x32 layers\n",
    "        h1_pool = tf.layers.max_pooling2d(h1, \n",
    "                              pool_size=(2, 2), \n",
    "                              strides=(2, 2))\n",
    "        # 2nd layer: Conv_2\n",
    "        # 12x12x32 -> 8x8x64 via 5x5x64 weight matrices \n",
    "        h2 = tf.layers.conv2d(h1_pool, kernel_size=(5,5), \n",
    "                              filters=64, \n",
    "                              activation=tf.nn.relu)\n",
    "        # MaxPooling \n",
    "        # Pooling w/stride produces 4x4x64 layers\n",
    "        h2_pool = tf.layers.max_pooling2d(h2, \n",
    "                              pool_size=(2, 2), \n",
    "                              strides=(2, 2))\n",
    "\n",
    "        # 3rd layer: Fully Connected (still relu activation)\n",
    "        # Look to condense 2nd pooling into 1024 element vector\n",
    "        # Here we implement Input -> Hidden with n_input = n_hidden\n",
    "        input_shape = h2_pool.get_shape().as_list()\n",
    "        n_input_units = np.prod(input_shape[1:])\n",
    "        h2_pool_flat = tf.reshape(h2_pool, \n",
    "                              shape=[-1, n_input_units])\n",
    "        h3 = tf.layers.dense(h2_pool_flat, 1024, \n",
    "                              activation=tf.nn.relu)\n",
    "\n",
    "        # Dropout\n",
    "        # Note the layers API has rate as a dropout argument\n",
    "        # rate = 1 - keep_probability\n",
    "        h3_drop = tf.layers.dropout(h3, \n",
    "                              rate=self.dropout_rate,\n",
    "                              training=is_train)\n",
    "        \n",
    "        # 4th layer: Fully Connected (linear activation)\n",
    "        # Implement Hidden -> Output with n_output = 10\n",
    "        h4 = tf.layers.dense(h3_drop, 10, \n",
    "                              activation=None)\n",
    "\n",
    "        # Prediction\n",
    "        # Dictionary to store prediction probabilties and labels\n",
    "        predictions = {\n",
    "            'probabilities': tf.nn.softmax(h4, \n",
    "                              name='probabilities'),\n",
    "            'labels': tf.cast(tf.argmax(h4, axis=1), \n",
    "                              tf.int32, name='labels')}\n",
    "        \n",
    "        # Loss Function and Optimization\n",
    "        # cross_entropy == Loss reduction via softmax (binary == logistic)\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=h4, labels=tf_y_onehot),\n",
    "            name='cross_entropy_loss')\n",
    "        \n",
    "        # Optimizer:\n",
    "        # Implement ADAM optimiser\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        optimizer = optimizer.minimize(cross_entropy_loss, name='train_op')\n",
    "\n",
    "        # Finding accuracy\n",
    "        correct_predictions = tf.equal(predictions['labels'], \n",
    "            tf_y, name='correct_preds')\n",
    "        \n",
    "        accuracy = tf.reduce_mean( # Reduce_mean == Mean\n",
    "            tf.cast(correct_predictions, tf.float32), # Casts as new type\n",
    "            name='accuracy')\n",
    "\n",
    "\n",
    "    # Saves a trained model for future use instead of rebuilding\n",
    "    def save(self, epoch, path='./tflayers-model/'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "        print('Saving model in %s' % path)\n",
    "        self.saver.save(self.sess, \n",
    "                        os.path.join(path, 'model.ckpt'),\n",
    "                        global_step=epoch)\n",
    "        \n",
    "    # Loads a saved model    \n",
    "    def load(self, epoch, path):\n",
    "        print('Loading model from %s' % path)\n",
    "        self.saver.restore(self.sess, \n",
    "             os.path.join(path, 'model.ckpt-%d' % epoch))\n",
    "        \n",
    "    # Trains the model once it is built by feeding in data   \n",
    "    # Data fed in as array containing feature set as 1st element, labels as 2nd\n",
    "    def train(self, training_set, \n",
    "              validation_set=None,\n",
    "              initialize=True):\n",
    "        # Initialize variables\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "\n",
    "        self.train_cost_ = []\n",
    "        X_data = np.array(training_set[0])\n",
    "        y_data = np.array(training_set[1])\n",
    "\n",
    "        # Standard embedded for-loops for epoch and batch iteration\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            # Generates n/64 batches of randomly sorted input data\n",
    "            batch_gen = \\\n",
    "                batch_generator(X_data, y_data, \n",
    "                                 shuffle=self.shuffle)\n",
    "            avg_loss = 0.0\n",
    "            for i, (batch_x,batch_y) in \\\n",
    "                enumerate(batch_gen):\n",
    "                # Dictionary containing shuffled batch\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': True} # For dropout\n",
    "                # Run computation graph to yeild loss\n",
    "                loss, _ = self.sess.run(\n",
    "                        ['cross_entropy_loss:0', 'train_op'], \n",
    "                        feed_dict=feed)\n",
    "                avg_loss += loss\n",
    "                \n",
    "            print('Epoch %02d: Training Avg. Loss: '\n",
    "                  '%7.3f' % (epoch, avg_loss), end=' ')\n",
    "            \n",
    "            # For Validation set training\n",
    "            if validation_set is not None:\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': False} # Dropout\n",
    "                valid_acc = self.sess.run('accuracy:0',\n",
    "                                          feed_dict=feed)\n",
    "                print('Validation Acc: %7.3f' % valid_acc)\n",
    "            else:\n",
    "                print()\n",
    "   \n",
    "    # Prediction on test set\n",
    "    def predict(self, X_test, return_proba = False):\n",
    "        feed = {'tf_x:0': X_test,\n",
    "                'is_train:0': False} # Dropout\n",
    "        if return_proba:\n",
    "            return self.sess.run('probabilities:0',\n",
    "                                 feed_dict=feed)\n",
    "        else:\n",
    "            return self.sess.run('labels:0',\n",
    "                                 feed_dict=feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building, Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instatiate a Conv model object and pass real data to the object, calling the train method to build and optmise our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Training Avg. Loss: 298.469 Validation Acc:   1.000\n",
      "Epoch 02: Training Avg. Loss:  81.535 Validation Acc:   0.984\n",
      "Epoch 03: Training Avg. Loss:  55.306 Validation Acc:   0.984\n",
      "Epoch 04: Training Avg. Loss:  43.297 Validation Acc:   0.953\n",
      "Epoch 05: Training Avg. Loss:  34.672 Validation Acc:   0.984\n",
      "Epoch 06: Training Avg. Loss:  30.366 Validation Acc:   1.000\n",
      "Epoch 07: Training Avg. Loss:  26.480 Validation Acc:   1.000\n",
      "Epoch 08: Training Avg. Loss:  23.269 Validation Acc:   1.000\n",
      "Epoch 09: Training Avg. Loss:  20.051 Validation Acc:   0.969\n",
      "Epoch 10: Training Avg. Loss:  18.355 Validation Acc:   1.000\n",
      "Epoch 11: Training Avg. Loss:  17.002 Validation Acc:   0.969\n",
      "Epoch 12: Training Avg. Loss:  15.142 Validation Acc:   0.984\n",
      "Epoch 13: Training Avg. Loss:  13.502 Validation Acc:   1.000\n",
      "Epoch 14: Training Avg. Loss:  12.651 Validation Acc:   0.969\n",
      "Epoch 15: Training Avg. Loss:  11.209 Validation Acc:   0.984\n",
      "Epoch 16: Training Avg. Loss:  10.227 Validation Acc:   1.000\n",
      "Epoch 17: Training Avg. Loss:   9.217 Validation Acc:   1.000\n",
      "Epoch 18: Training Avg. Loss:   8.962 Validation Acc:   1.000\n",
      "Epoch 19: Training Avg. Loss:   8.692 Validation Acc:   1.000\n",
      "Epoch 20: Training Avg. Loss:   7.615 Validation Acc:   1.000\n",
      "Saving model in ./tflayers-model/\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNN(random_seed=123)\n",
    "\n",
    "cnn.train(training_set=(X_train_norm, y_train), \n",
    "          validation_set=(X_valid_norm, y_valid))\n",
    "\n",
    "# Save parameters from above model for future use\n",
    "cnn.save(epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use the model to predict new classes given our test set for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = cnn.predict(X_test_norm)\n",
    "\n",
    "submission = pd.DataFrame({'ImageId' : index.loc[:,'ImageId'], 'Label' : subm.T})\n",
    "submission.to_csv('submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
